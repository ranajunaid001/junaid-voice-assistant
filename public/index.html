<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Voice Assistant</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'SF Pro Display', sans-serif;
            background: #FFFFFF;
            color: #000000;
            height: 100vh;
            display: flex;
            align-items: center;
            justify-content: center;
            -webkit-font-smoothing: antialiased;
            -moz-osx-font-smoothing: grayscale;
        }

        .container {
            text-align: center;
            padding: 40px;
            max-width: 600px;
        }

        .orb {
            width: 120px;
            height: 120px;
            margin: 0 auto 40px;
            background: #000000;
            border-radius: 50%;
            cursor: pointer;
            transition: all 0.3s cubic-bezier(0.25, 0.46, 0.45, 0.94);
            position: relative;
            overflow: hidden;
        }

        .orb::before {
            content: '';
            position: absolute;
            top: 50%;
            left: 50%;
            width: 0;
            height: 0;
            border-radius: 50%;
            background: rgba(255, 255, 255, 0.1);
            transform: translate(-50%, -50%);
            transition: all 0.6s;
        }

        .orb.connected {
            background: #007AFF;
        }

        .orb.listening {
            background: #34C759;
        }

        .orb.speaking {
            background: #5856D6;
        }

        .orb.listening::before,
        .orb.speaking::before {
            animation: pulse 1.5s ease-out infinite;
        }

        @keyframes pulse {
            0% {
                width: 0;
                height: 0;
                opacity: 0.8;
            }
            100% {
                width: 200%;
                height: 200%;
                opacity: 0;
            }
        }

        .status {
            font-size: 14px;
            color: #8E8E93;
            font-weight: 400;
            letter-spacing: -0.02em;
            margin-bottom: 8px;
            min-height: 20px;
        }

        .instruction {
            font-size: 17px;
            color: #000000;
            font-weight: 500;
            letter-spacing: -0.02em;
            margin-bottom: 60px;
        }

        /* Waveform visualization */
        .visualizer {
            width: 300px;
            height: 60px;
            margin: 0 auto 40px;
            display: flex;
            align-items: center;
            justify-content: center;
            gap: 2px;
            opacity: 0;
            transition: opacity 0.3s;
        }

        .visualizer.active {
            opacity: 1;
        }

        .bar {
            width: 3px;
            background: #000000;
            border-radius: 2px;
            transition: height 0.1s ease;
        }

        /* Connection state */
        .connection-indicator {
            position: fixed;
            top: 40px;
            right: 40px;
            display: flex;
            align-items: center;
            gap: 8px;
            font-size: 12px;
            color: #8E8E93;
        }

        .connection-dot {
            width: 8px;
            height: 8px;
            border-radius: 50%;
            background: #8E8E93;
            transition: background 0.3s;
        }

        .connection-dot.connected {
            background: #34C759;
        }

        /* Transcript */
        .transcript {
            margin-top: 40px;
            padding: 20px;
            background: #F2F2F7;
            border-radius: 12px;
            min-height: 100px;
            text-align: left;
            font-size: 14px;
            line-height: 1.6;
            color: #000000;
            opacity: 0;
            transition: opacity 0.3s;
        }

        .transcript.show {
            opacity: 1;
        }

        .transcript-label {
            font-size: 11px;
            color: #8E8E93;
            text-transform: uppercase;
            letter-spacing: 0.05em;
            margin-bottom: 8px;
        }

        .message {
            margin: 8px 0;
            padding: 8px 12px;
            border-radius: 8px;
        }

        .message.user {
            background: #007AFF;
            color: white;
            margin-left: 40px;
            text-align: right;
        }

        .message.assistant {
            background: white;
            margin-right: 40px;
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="connection-indicator">
            <div class="connection-dot" id="connectionDot"></div>
            <span id="connectionText">Disconnected</span>
        </div>

        <div class="status" id="status">Initializing</div>
        <div class="instruction" id="instruction">Click to start conversation</div>
        
        <div class="orb" id="orb"></div>
        
        <div class="visualizer" id="visualizer">
            <!-- Audio bars will be inserted here -->
        </div>

        <div class="transcript" id="transcript">
            <div class="transcript-label">Conversation</div>
            <div id="messages"></div>
        </div>
    </div>

    <script>
        let ws = null;
        let audioContext = null;
        let mediaStream = null;
        let processor = null;
        let isConnected = false;
        let isActive = false;
        let currentAudio = null;

        const orb = document.getElementById('orb');
        const status = document.getElementById('status');
        const instruction = document.getElementById('instruction');
        const connectionDot = document.getElementById('connectionDot');
        const connectionText = document.getElementById('connectionText');
        const visualizer = document.getElementById('visualizer');
        const transcript = document.getElementById('transcript');
        const messages = document.getElementById('messages');

        // Create visualizer bars
        for (let i = 0; i < 40; i++) {
            const bar = document.createElement('div');
            bar.className = 'bar';
            bar.style.height = '4px';
            visualizer.appendChild(bar);
        }

        // Initialize WebSocket connection
        function connectWebSocket() {
            // Update this to your Railway WebSocket URL
            const wsUrl = window.location.protocol === 'https:' 
                ? `wss://${window.location.host}/ws`
                : `ws://${window.location.host}/ws`;

            ws = new WebSocket(wsUrl);

            ws.onopen = () => {
                console.log('WebSocket connected');
                isConnected = true;
                connectionDot.classList.add('connected');
                connectionText.textContent = 'Connected';
                status.textContent = 'Ready';
                orb.classList.add('connected');
            };

            ws.onmessage = (event) => {
                const data = JSON.parse(event.data);
                handleServerMessage(data);
            };

            ws.onerror = (error) => {
                console.error('WebSocket error:', error);
                status.textContent = 'Connection error';
            };

            ws.onclose = () => {
                console.log('WebSocket disconnected');
                isConnected = false;
                connectionDot.classList.remove('connected');
                connectionText.textContent = 'Disconnected';
                status.textContent = 'Disconnected';
                orb.classList.remove('connected', 'listening', 'speaking');
                
                // Reconnect after 3 seconds
                setTimeout(connectWebSocket, 3000);
            };
        }

        // Handle messages from server
        function handleServerMessage(data) {
            switch (data.type) {
                case 'transcript':
                    addMessage(data.text, 'user', data.final);
                    break;
                
                case 'response':
                    addMessage(data.text, 'assistant');
                    break;
                
                case 'audio':
                    playAudioChunk(data);
                    break;
                
                case 'state':
                    updateState(data.state);
                    break;
                    
                case 'command':
                    if (data.action === 'stopAudio') {
                        // Stop current audio
                        if (currentAudio) {
                            currentAudio.pause();
                            currentAudio = null;
                        }
                        console.log('Audio playback stopped due to interruption');
                    }
                    break;
            }
        }

        // Add message to transcript
        function addMessage(text, sender, isFinal = true) {
            transcript.classList.add('show');
            
            let lastMessage = messages.lastElementChild;
            
            if (!isFinal && lastMessage && lastMessage.dataset.sender === sender) {
                // Update existing message
                lastMessage.textContent = text;
            } else {
                // Create new message
                const message = document.createElement('div');
                message.className = `message ${sender}`;
                message.dataset.sender = sender;
                message.textContent = text;
                messages.appendChild(message);
                messages.scrollTop = messages.scrollHeight;
            }
        }

        // Update UI state
        function updateState(state) {
            orb.className = 'orb connected';
            
            switch (state) {
                case 'listening':
                    orb.classList.add('listening');
                    status.textContent = 'Listening';
                    visualizer.classList.add('active');
                    break;
                
                case 'processing':
                    status.textContent = 'Processing';
                    visualizer.classList.remove('active');
                    break;
                
                case 'speaking':
                    orb.classList.add('speaking');
                    status.textContent = 'Speaking';
                    visualizer.classList.add('active');
                    break;
                
                case 'idle':
                    status.textContent = 'Ready';
                    visualizer.classList.remove('active');
                    break;
            }
        }

        // Initialize audio context and microphone
        async function initAudio() {
            try {
                audioContext = new (window.AudioContext || window.webkitAudioContext)({ sampleRate: 16000 });
                mediaStream = await navigator.mediaDevices.getUserMedia({ 
                    audio: {
                        sampleRate: 16000,
                        channelCount: 1,
                        echoCancellation: true,
                        noiseSuppression: true
                    } 
                });
                
                const source = audioContext.createMediaStreamSource(mediaStream);
                processor = audioContext.createScriptProcessor(2048, 1, 1);
                
                let messageCount = 0;
                
                processor.onaudioprocess = (e) => {
                    if (!isActive || !isConnected || !ws || ws.readyState !== WebSocket.OPEN) return;
                    
                    console.log('Audio process fired!'); // ADD THIS LINE
                    
                    const inputData = e.inputBuffer.getChannelData(0);
                    const outputData = new Int16Array(inputData.length);
                    
                    // Convert float32 to int16
                    for (let i = 0; i < inputData.length; i++) {
                        const s = Math.max(-1, Math.min(1, inputData[i]));
                        outputData[i] = s < 0 ? s * 0x8000 : s * 0x7FFF;
                    }
                    
                    // Send audio data via WebSocket
                    try {
                        ws.send(JSON.stringify({
                            type: 'audio',
                            data: Array.from(outputData)
                        }));
                        
                        // Debug log every 50th message
                        messageCount++;
                        if (messageCount % 50 === 0) {
                            console.log(`Sent ${messageCount} audio chunks`);
                        }
                    } catch (error) {
                        console.error('Error sending audio:', error);
                    }
                    
                    // Update visualizer
                    updateVisualizer(inputData);
                };
                
                source.connect(processor);
                processor.connect(audioContext.destination);
                
                instruction.textContent = 'Click to start conversation';
                console.log('Audio initialized successfully');
                
            } catch (error) {
                console.error('Error initializing audio:', error);
                status.textContent = 'Microphone access required';
            }
        }

        // Update audio visualizer
        function updateVisualizer(data) {
            const bars = visualizer.querySelectorAll('.bar');
            const step = Math.floor(data.length / bars.length);
            
            for (let i = 0; i < bars.length; i++) {
                const value = Math.abs(data[i * step]);
                const height = Math.max(4, Math.min(60, value * 200));
                bars[i].style.height = `${height}px`;
            }
        }

        // Play audio chunk from server
        async function playAudioChunk(audioData) {
            try {
                // Stop any currently playing audio
                if (currentAudio) {
                    currentAudio.pause();
                    currentAudio = null;
                }
                
                // Convert base64 to blob
                const binaryString = atob(audioData.data);
                const bytes = new Uint8Array(binaryString.length);
                for (let i = 0; i < binaryString.length; i++) {
                    bytes[i] = binaryString.charCodeAt(i);
                }
                
                const blob = new Blob([bytes], { type: 'audio/wav' });
                const url = URL.createObjectURL(blob);
                
                // Create and play audio
                currentAudio = new Audio(url);
                currentAudio.play();
                
                // Clean up when audio ends
                currentAudio.addEventListener('ended', () => {
                    URL.revokeObjectURL(url);
                    currentAudio = null;
                });
                
                console.log('Playing TTS audio');
            } catch (error) {
                console.error('Error playing audio:', error);
                currentAudio = null;
            }
        }

        // Toggle conversation
        function toggleConversation() {
            if (!isConnected || !audioContext) {
                console.error('Not ready:', { isConnected, hasAudioContext: !!audioContext });
                return;
            }
            
            isActive = !isActive;
            
            if (isActive) {
                console.log('Starting conversation');
                ws.send(JSON.stringify({ type: 'start' }));
                instruction.textContent = 'Click to stop';
                
                // Resume audio context if suspended
                if (audioContext.state === 'suspended') {
                    audioContext.resume();
                }
            } else {
                console.log('Stopping conversation');
                ws.send(JSON.stringify({ type: 'stop' }));
                instruction.textContent = 'Click to start conversation';
            }
        }

        // Event listeners
        orb.addEventListener('click', toggleConversation);

        // Initialize
        connectWebSocket();
        initAudio();

        // Show initializing state
        status.textContent = 'Initializing';
    </script>
</body>
</html>
